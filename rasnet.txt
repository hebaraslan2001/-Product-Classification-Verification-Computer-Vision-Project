import os
from turtle import pd
import cv2
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from tqdm import tqdm
from random import shuffle
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense, Dropout, Input, Add
from keras.callbacks import ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from keras import Model
class_names = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19','20' ] 
class_names_label = {class_name: i for i, class_name in enumerate(class_names)}
nb_classes = len(class_names)
print(class_names_label) 

def load_data():
    DIRECTORY = r"C:/Users/ALSHARKAOY/Desktop/" 
    CATEGORY = ["input", "Validation"]
    output = []
    image_size=(160,160)
    for category in CATEGORY:
        path = os.path.join(DIRECTORY, category)
        print(path)
        images = [] 
        labels =[]
        print("Loading {}".format(category))
        for folder in os.listdir(path):
            label = class_names_label[folder]
            # Iterate through each image in our folder for file in os.listdir(os.path.join(path, folder)):
            for file in os.listdir(os.path.join(path,folder)):
                # Get the path name of the image
                img_path = os.path.join(os.path.join(path, folder), file)
                # Open and resize the img
               # print(img_path)
                
                image = cv2.imread(img_path)
                #plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
                #plt.show()
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                image = cv2.resize(image, image_size)
                
                images.append(image)
                labels.append(label)
        images=np.array(images, dtype = 'float32')
        lables=np.array(labels, dtype = 'int32')
        output.append((images,labels))
    return output        


# Load data
(train_images, train_labels), (test_images, test_labels) = load_data()

# Initialize ImageDataGenerator for augmentation
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Fit the ImageDataGenerator on your training data
train_datagen.fit(train_images)

# Use flow method to generate augmented images
# Set the batch size and other parameters for generating augmented images
augmented_images = []
augmented_labels = []

# Generate more samples using augmentation
for X_batch, y_batch in train_datagen.flow(train_images, train_labels, batch_size=32, shuffle=True):
    augmented_images.extend(X_batch)
    augmented_labels.extend(y_batch)
    if len(augmented_images) >= 10000:  # Generate a larger dataset, adjust as needed
        break

# # Convert lists to numpy arrays
# augmented_images = np.array(augmented_images)
# augmented_labels = np.array(augmented_labels)

# # Combine the original training data with augmented data
# train_images = np.concatenate((train_images, augmented_images))
# train_labels = np.concatenate((train_labels, augmented_labels))


from sklearn.model_selection import train_test_split
import numpy as np

# Convert lists to numpy arrays
augmented_images = np.array(augmented_images)
augmented_labels = np.array(augmented_labels)

# Combine the original training data with augmented data
train_images = np.concatenate((train_images, augmented_images))
train_labels = np.concatenate((train_labels, augmented_labels))

# Convert lists to numpy arrays for the original test data
test_images = np.array(test_images)
test_labels = np.array(test_labels)

# Split augmented data into training and validation sets
train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)
from keras.applications import ResNet50
from keras.layers import GlobalAveragePooling2D

# Load the ResNet50 model without the top (fully connected) layers
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(160, 160, 3))

# Add custom layers on top of the ResNet50 base
model = Sequential()
model.add(base_model)
model.add(GlobalAveragePooling2D())
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(nb_classes, activation='softmax'))  # nb_classes is the number of classes in your data

# Freeze the layers of the ResNet50 base (optional)
for layer in base_model.layers:
    layer.trainable = False

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(train_images, train_labels, epochs=10, batch_size=32, validation_data=(val_images, val_labels))

# Evaluate the model on test data
test_loss, test_accuracy = model.evaluate(val_images, val_labels)
print(f"Test accuracy: {test_accuracy}")
